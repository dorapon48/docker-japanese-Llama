# ELYZA-japanese-Llama-2-7b-fast-instructをダウンロードする
FROM nvidia/cuda:12.0.0-cudnn8-devel-ubuntu20.04

# huggingfaceからダウンロード
WORKDIR /opt
RUN ln -sf /usr/share/zoneinfo/Asia/Tokyo /etc/localtime
RUN sed -i.bak -e "s%http://archive.ubuntu.com/ubuntu/%http://ftp.iij.ad.jp/pub/linux/ubuntu/archive/%g" /etc/apt/sources.list

RUN apt-get update \
    && apt-get -y install cmake git python3 python3-pip
RUN pip install huggingface-hub

# # llama.cppを準備
WORKDIR /opt
RUN git clone https://github.com/ggerganov/llama.cpp.git
WORKDIR /opt/llama.cpp
RUN make LLAMA_CUBLAS=1
RUN pip install -r requirements.txt

WORKDIR /download
COPY . /download
RUN chmod +x /download/download.sh
#CMD [ "/download/download.sh" ]
# RUN python3 download.py

# # ELYZA-japanese-Llama-2-7b-fast-instructをguffファイル化する
# RUN ln -sf /usr/share/zoneinfo/Asia/Tokyo /etc/localtime
# # 日本のミラーサイトに変更(iij)
# RUN sed -i.bak -e "s%http://archive.ubuntu.com/ubuntu/%http://ftp.iij.ad.jp/pub/linux/ubuntu/archive/%g" /etc/apt/sources.list

# RUN apt-get update \
#     && apt-get -y install cmake git python3 python3-pip

# # llama.cppを準備
# WORKDIR /opt
# RUN git clone https://github.com/ggerganov/llama.cpp.git
# WORKDIR /opt/llama.cpp
# RUN make LLAMA_CUBLAS=1
# RUN pip install -r requirements.txt

# # guff化
# WORKDIR /data/download
# COPY --from=downloader data/download/ELYZA-model /opt/ELYZA-model
# RUN python3 /opt/llama.cpp/convert-llama-ggml-to-gguf.py --input ELYZA-model --output ELYZA-model.gguf
