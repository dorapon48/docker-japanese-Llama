# ELYZA-japanese-Llama-2-7b-fast-instructをダウンロードする
FROM ubuntu:20.04 AS downloader

# huggingfaceからダウンロード
WORKDIR /download
RUN ln -sf /usr/share/zoneinfo/Asia/Tokyo /etc/localtime
RUN sed -i.bak -e "s%http://archive.ubuntu.com/ubuntu/%http://ftp.iij.ad.jp/pub/linux/ubuntu/archive/%g" /etc/apt/sources.list

RUN apt-get update \
    && apt-get -y install python3 python3-pip
RUN pip install huggingface-hub

COPY download.py .
RUN python3 download.py
COPY /download/ELYZA-model /app/download

# ELYZA-japanese-Llama-2-7b-fast-instructをguffファイル化する
FROM nvidia/cuda:12.0.0-cudnn8-devel-ubuntu20.04 AS converter

RUN ln -sf /usr/share/zoneinfo/Asia/Tokyo /etc/localtime
# 日本のミラーサイトに変更(iij)
RUN sed -i.bak -e "s%http://archive.ubuntu.com/ubuntu/%http://ftp.iij.ad.jp/pub/linux/ubuntu/archive/%g" /etc/apt/sources.list

RUN apt-get update \
    && apt-get -y install cmake git python3 python3-pip

# llama.cppを準備
WORKDIR /opt
RUN git clone https://github.com/ggerganov/llama.cpp.git
WORKDIR /opt/llama.cpp
RUN make LLAMA_CUBLAS=1
RUN pip install -r requirements.txt

WORKDIR /opt
COPY --from=downloader /download/ELYZA-model /opt/ELYZA-model
RUN python3 /opt/llama.cpp/convert-llama-ggml-to-gguf.py --input ELYZA-model --output ELYZA-model.gguf

FROM nvidia/cuda:12.0.0-cudnn8-devel-ubuntu20.04

ARG BASE_PATH=/app

ENV HOST 0.0.0.0
EXPOSE 8000

# optは動作しないので注意
WORKDIR $BASE_PATH

RUN ln -sf /usr/share/zoneinfo/Asia/Tokyo /etc/localtime
# 日本のミラーサイトに変更(iij)
RUN sed -i.bak -e "s%http://archive.ubuntu.com/ubuntu/%http://ftp.iij.ad.jp/pub/linux/ubuntu/archive/%g" /etc/apt/sources.list

RUN apt-get update \
    && apt-get -y install cmake git python3 python3-pip

ARG CMAKE_ARGS="-DLLAMA_CUBLAS=on"
ARG FORCE_CMAKE=1
RUN pip install llama-cpp-python==0.2.19
RUN pip install llama-cpp-python[server]

# llama.cppを準備
# WORKDIR /opt
# RUN git clone https://github.com/ggerganov/llama.cpp.git
# WORKDIR /opt/llama.cpp
# RUN make LLAMA_CUBLAS=1
# RUN pip install -r requirements.txt

# ダウンロードしたモデルを量子化
WORKDIR /opt
COPY --from=downloader /download/ELYZA-model /opt/ELYZA-model
# RUN python3 llama.cpp/convert.py ELYZA-model --outfile ELYZA-model.gguf
COPY --from=converter /opt/ELYZA-model.gguf /app/model/

WORKDIR $BASE_PATH