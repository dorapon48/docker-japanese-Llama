2023/12/21
dockerでELYZA-japanese-Llama-2-7bを使えるようにしてみる。
huggingface-hubとやらからダウンロードしてみる
elyza/ELYZA-japanese-Llama-2-7b-fast-instruct
ダウンロードと最適化を行ったやつを別イメージで作成し、完成したggufファイルをコピーするのがよさそう
とりあえず必要そうなものはそろったか？

2023/12/22
GPUを使うにはいろいろとめんどくさそうだったが、
pytorchの何かがありそう

2023/12/23
おっもいんでnvidia/cuda:12.3.1-devel-ubuntu20.04を使う
お、いろいろやってたら行けた
とりあえず、nvidia/cuda:12.0.0-cudnn8-runtime-ubuntu20.04を使う、PCのcudaのバージョンと合わせた方がいいらしい
少し低いバージョンを使えばいいのかも
うまくいった！
/optはnividiaが使うみたいなので/appに変更すること
よし、サーバーを立ててみる
llama.cppとダウンロードは別にする
なんかよくわかんないCOPYエラーが発生している
エラーは表示されていないが、ファイルが存在しなくなる。場所の問題か？
場所の問題だったらしい、マウントするところにコピーしていたのでおかしくあとから上書きしていたのかな
マウントするには最新の注意を払おう
ダウンロード系はoptに突っ込んどく
dockerfileを書き換えるともうダウンロードが走ると考えていいのでしばらくコメントアウトしてもいいかもしれない

2023/12/24
/opt/llama.cpp/server -m /app/llama.cpp/models/llama-2-7b-chat.ggmlv3.q2_K.gguf -c 2048 --host localhost --port 30000
サーバーを作るの面倒みたいねどうにかせねば

2024/01/18
pythonのバージョンが古い可能性がある
ちがうっぽい
llama-cpp-pythonのバージョンを下げたらいけた
dekita!
ポートのエラーが出ていたが、0.0.0.0に設定することでlocalhostでつながるようになった
python3 -m llama_cpp.server --model llama.cpp/models/llama-2-7b-chat.ggmlv3.q2_K.gguf
Llama.cppをダウンロードする必要がなくなったためdownloaderの方で処理してからと思ったが、
そもそも処理済みのものがあるらしいのでそれを持ってきた方が楽
と思ったがちょっと怖いので真面目にする

2024/01/19
全てを1つのdockerファイルでやるのは無理があると思うので、ダウンロード専用のdockerfileを作成する
shファイルにまとめることでbuild時間を少なくしているらしい
僕もそうしよう
...マウントしているところにコピーするのはやめよう
動作をdownloadディレクトリですることにした
shファイルの一番上の部分は、#!/usr/bin/env bashにしておくのが無難

2024/01/21
convert.pyの処理がうまくいかない
どうも、tokenizer.modelが見当たらないほかのelyzaのやつには入っているのでそれが問題かも
elyza/ELYZA-japanese-Llama-2-7b-fast-instruct、elyza/ELYZA-japanese-Llama-2-7b-fastには入っていない
elyza/ELYZA-japanese-Llama-2-7b-instructで試してみる
.envファイルを用意して、ダウンロードを制御する
fileサイズが大きすぎるので、通信環境がいいとこでやる

2024/01/22
何とかなった
docker-compose --profile download up --build
でダウンロードできる
.envファイルにhuggingfaceのidと、保存するファイルの名前を入力すれば動くはず
次は、mainの方

2024/01/23
curl -s -XPOST -H 'Content-Type: application/json' localhost:8000/v1/chat/completions -d '{"messages": [{"role": "user", "content": "Could you introduce yourself?"}]}'
実験
curl -X 'POST' \
  'http://localhost:8000/v1/chat/completions' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "messages": [
    {
      "content": "富士山は何メートル？",
      "role": "user"
    }
  ]
}'
linux版のcurlですること
python3 -m llama_cpp.server --model /data/models/ELYZA-japanese-Llama-2-7b-instruct.gguf --n_gpu_layers -1
--n_gpu_layers -1はgpuに合わせるんじゃなくて、全部gpuにぶち込むみたいな設定みたいね
out of memね
2で772.06MB
大体
モデルの大きさ / config.jsonのnum_hidden_layers = --n_gpu_layers 1
が出る
例12.55(GB) * 1024 / 32 = 401.6(MB)
そして、ギリギリまで使おうとするとout of memしてしまうので、注意
n_gpu_layersは.envファイルで帰れるようにしましょう
コンテナ名とか別のにしとくかね

2024/01/24
モデルの大きさは8bit量子化で半分ぐらいになった。
outtypeで指定
毎回ビルドしなくてもいいようにした。
これでおっけいのはず
ただ、gpuを最大限使うのはggufの形式ではないので、gtpqやawqを使ってみる方がいいかもしれない
ただ、全部gpuに突っ込んだ時の挙動は試したことがないので要検証

https://github.com/oobabooga/text-generation-webui?tab=readme-ov-file
https://github.com/casper-hansen/AutoAWQ
ちょっとawqを試してみる
追加したもの
git clone https://github.com/mit-han-lab/llm-awq
apt-get install software-properties-common
add-apt-repository ppa:deadsnakes/ppa -y
apt-get install python3.10
pip install --upgrade pip
...なんだか意味不明の仕様になってるようだもう使わんautoawqで何とかする
うーん...性能不足なのかプログラムがkillされる
どうにもならんのでggufで進める

CMD [ "bash", "/download/download.sh" ]
ちゃんとbashを書いてないと動かないらしい

2024/02/19
使いやすいように変更する
環境変数を減らした，あとlogを表示するように変更
優先ダウンロードを変更などした
shに例外処理を追加
